{"name":"Dplyr-tidyr-tutorial","tagline":"Tutorial on dplyr and tidyr packages for UCSB's Eco-Data-Science group","body":"Overview\r\n--------\r\n\r\n> Data scientists, according to interviews and expert estimates, spend\r\n> from 50 percent to 80 percent of their time mired in the mundane labor\r\n> of collecting and preparing data, before it can be explored for useful\r\n> information. - [NYTimes\r\n> (2014)](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)\r\n\r\nThis tutorial will cover the `tidyr` and `dplyr` packages created by the\r\nmythical code wizard [Hadley Wickham](https://github.com/hadley) of\r\n`ggplot2` fame. The \"gg\" in `ggplot2` stands for the \"grammar of\r\ngraphics\". Hadley similarly considers the functionality of the two\r\npackages `dplyr` and `tidyr` to provide the \"grammar of data\r\nmanipulation\". The following topics will be covered:\r\n\r\n### Resources\r\n\r\n-   [Data wrangling cheatsheet\r\n    (`dplyr`,`tidyr`)](http://ucsb-bren.github.io/refs/cheatsheets/data-wrangling-cheatsheet.pdf)\r\n-   [Data wrangling with R and\r\n    RStudio](https://www.rstudio.com/resources/webinars/data-wrangling-with-r-and-rstudio/)\r\n-   [slides](http://ucsb-bren.github.io/env-info/wk03_dplyr/wrangling-webinar.pdf)\r\n-   [dplyr vignette: Introduction to\r\n    dplyr](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)\r\n-   [Two-table\r\n    verbs](https://cran.rstudio.com/web/packages/dplyr/vignettes/two-table.html)\r\n-   [Window functions and grouped\r\n    mutate/filter](https://cran.rstudio.com/web/packages/dplyr/vignettes/window-functions.html)\r\n-   [Databases](https://cran.rstudio.com/web/packages/dplyr/vignettes/databases.html)\r\n-   [Non-standard\r\n    evaluation](https://cran.rstudio.com/web/packages/dplyr/vignettes/nse.html)\r\n-   [tidyr vignette: Tidy\r\n    data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)\r\n-   [Introduction to dplyr for Faster Data Manipulation in\r\n    R](https://rpubs.com/justmarkham/dplyr-tutorial)\r\n-   [Environmental Informatics |\r\n    ucsb-bren/env-info](http://ucsb-bren.github.io/env-info/)\r\n-   bigrquery tutorials:\r\n-   [A new data processing workflow for R: dplyr, magrittr, tidyr,\r\n    ggplot2 | Technical Tidbits From Spatial Analysis & Data\r\n    Science](http://zevross.com/blog/2015/01/13/a-new-data-processing-workflow-for-r-dplyr-magrittr-tidyr-ggplot2/) (newer)\r\n-   [Fetching BigQuery\r\n    Data](http://dtkaplan.github.io/CVC/Data/Birthdays/Birthdays.html)\r\n\r\n### Getting Started\r\n\r\nYou can [download\r\nRStudio](https://www.rstudio.com/products/rstudio/download/) if you\r\ndon't have latest version `0.99.892` (menu RStudio -&gt; About RStudio),\r\nwhich has many nice additions for running R chunks and providing table\r\nof contents in Rmarkdown documents.\r\n\r\nInstall and/or load the following packages:\r\n\r\n    ## Install packages if needed\r\n    # install.packages('devtools')\r\n    # install.packages('readr')\r\n    # install.packages('dplyr')\r\n    # install.packages('tidyr')\r\n    # install.packages('stringr')\r\n    # install.packages('ggplot2')\r\n\r\n    # Load packages\r\n    library(devtools)\r\n    library(readr)\r\n    # library(plyr)\r\n    library(dplyr)\r\n    library(broom)\r\n    library(tidyr)\r\n    library(stringr)\r\n    library(ggplot2)\r\n\r\n    # Check package versions after Packages pane -> Update\r\n    devtools::session_info()\r\n\r\n### Why use dplyr and tidyr?\r\n\r\n1.  **Speed** - dplyr and tidyr are *really* fast  \r\n2.  **Readability** - the code syntax is straightforward and easy to\r\n    read  \r\n3.  **Chaining** - *never break the chain*. More on this later  \r\n4.  **Integrates with ggplot2** - plot your data in the same workflow\r\n    that you manipulate it with\r\n5.  **Can be used to analyze external databases without knowledge of\r\n    additional database query languages**\r\n\r\nBasics of dplyr and tidyr\r\n-------------------------\r\n\r\n### Data frames and data tables\r\n\r\nAlthough technically two separate packages, **dplyr** and **tidyr** were\r\ndesigned to work together and can basically be thought of as a single\r\npackage. They are designed to work with data frames as is, but it is\r\ngenerally a good idea to convert your data to table data using the\r\n`read_csv()` or `tbl_df()` functions, particularly when working with\r\nlarge datasets.\r\n\r\n    ## Comparing read.csv with read_csv\r\n    # Read in FAO data\r\n    fao   <- read.csv(file = 'data/FAO_1950to2012_111914.csv', stringsAsFactors = F) \r\n    summary(fao)\r\n    head(fao)\r\n    # vs using read_csv\r\n    fao   <- read_csv(file = 'data/FAO_1950to2012_111914.csv') \r\n    fao\r\n    # note: read_csv like read.csv(...)\r\n    #       also keeps original column names and converts to tbl_df()\r\n    names(fao) = make.names(names(fao), unique=T) # since original column names have duplicates\r\n\r\n    ## Consider what happens with the following command\r\n    # fao # all entries are printed in your console\r\n    head(fao) # top five entries are printed in your console, columns wrap and can be difficult to follow if working with many variables\r\n    summary(fao)\r\n\r\n    ## With dplyr\r\n    fao<-tbl_df(fao) # convert to table data\r\n    fao # now top 10 rows are shown along with data type of each variable. Variables that do not fit in console window are shown below.\r\n    glimpse(fao) # view all columns \r\n    summary(fao)\r\n    if (interactive()) View(fao) # interactive==T if in Console, not knitting\r\n\r\n### Tidy data\r\n\r\nIn general, it is good practice to have your data organized in a \"tidy\"\r\nformat.\r\n\r\nIn tidy data:\r\n\r\n-   Each variable forms a column  \r\n-   Each observation forms a row  \r\n-   Each type of observational unit forms a table\r\n\r\n### Main verbs of dplyr and tidyr\r\n\r\nTidyr and dplyr are designed to help manipulate data sets, allowing you\r\nto convert between *wide* and *long* formats, fill in missing values and\r\ncombinations, separate or merge multiple columns, rename and create new\r\nvariables, and summarize data according to grouping variables.\r\n\r\nDplyr and tidyr rely on the following main verbs:\r\n\r\n-   Tidyr\r\n-   `gather()` and `spread()` convert data between wide and long\r\n    format  \r\n-   `separate()` and `unite()` separate a single column into multiple\r\n    columns and vice versa  \r\n-   `complete()` turns implicit missing values in explicit missing\r\n    values by completing missing data combinations\r\n\r\n-   Dplyr\r\n-   `filter()` subset data based on logical criteria  \r\n-   `select()` select certain columns  \r\n-   `arrange()` order rows by value of a column  \r\n-   `rename()` rename columns  \r\n-   `group_by()` group data by common variables for performing\r\n    calculations  \r\n-   `mutate()` create a new variable/column  \r\n-   `summarize()` summarize data into a single row of values\r\n\r\nNote that *unquoted* variable names are used by default in tidyr and\r\ndplyr functions.\r\n\r\nWe'll use these verbs to process the raw FAO landings data into a more\r\nmanageable tidy format.\r\n\r\n#### Gather and Spread\r\n\r\nFirst let's convert the FAO data from the current wide format to a long\r\nformat.\r\n\r\n    # Let's convert the fao data from it's current wide format to a long format using gather(). Note the use of helper fnc\r\n    d <- gather(fao, key='Year', value='Catch', num_range('X',1950:2012)) # ?select for num_range()\r\n\r\n    # We can convert back to wide format with the spread function by calling the previously created variables\r\n    spread(d,Year, Catch)\r\n\r\n    if (interactive()) View(d) # interactive==T if in Console, not knitting\r\n    # to handle: '-','...',' F','X'\r\n\r\n#### Rename\r\n\r\nNow let's rename the columns to more manageable names (syntax is *new\r\nname* = *old name*)\r\n\r\n    # Note the use of backticks around column names with special characters like \"(\"\r\n    d <- dplyr::rename(d,\r\n              country     = Country..Country.,\r\n              commname    = Species..ASFIS.species.,\r\n              sciname     = Species..ASFIS.species..2,\r\n              spcode      = Species..ASFIS.species..1,\r\n              spgroup     = Species..ISSCAAP.group.,\r\n              spgroupname = Species..ISSCAAP.group..1,\r\n              regionfao   = Fishing.area..FAO.major.fishing.area.,\r\n              unit        = Measure..Measure.,\r\n              year        = Year,catch=Catch)\r\n\r\n#### Select\r\n\r\nRemove unwanted columns and observations.\r\n\r\n    # we could chose all the columns to keep\r\n    select(d,country, commname, sciname, spcode, spgroupname, regionfao, year, catch)\r\n\r\n    # but it's easier to just specify the columns to get rid of\r\n    d<-select(d,-spgroup,-unit)\r\n\r\nThere are also a number of **helper functions** that can be used in\r\nconjunction with `select()` to let you select without individually\r\nlisting all those you wish to keep or drop. We used a helper function\r\npreviously in our `gather()` function and now we'll try a few others.\r\n\r\n    # select all coloumns that begin with the letter s\r\n    select(d, starts_with('s'))\r\n\r\n    # select columns that match a regular expression\r\n    select(d, matches('*name'))\r\n\r\n    # select columns between two columns by referencing their position like normal [,x:y] syntax \r\n    select(d, country, spcode:year)\r\n\r\n    # select every column (though I haven't found a situation where this is useful yet...)\r\n    select(d,everything())\r\n\r\n#### Arrange\r\n\r\nArrange entries by country, scientific name, fao region and year. You\r\ncan use `desc()` within `arrange()` to control which variables you want\r\nto order in ascending or descending fashion\r\n\r\n    # arrange by country, sciname, regionfao, and year\r\n    d<-arrange(d,country,sciname,regionfao,year)\r\n\r\n    # if we'd like the years to be descending\r\n    arrange(d, country, desc(sciname), regionfao, desc(year))\r\n\r\n    # if we want to first order by species\r\n    arrange(d, sciname, country, regionfao, year)\r\n\r\n#### Mutate\r\n\r\nMutate can be used to edit existing variables or create new ones.\r\n\r\n    d <- mutate(d,\r\n                year      = as.numeric(str_replace(year, 'X', '')), # strip X off all year values and convert to numeric\r\n                catch     = as.numeric(str_replace(catch, c(' F','...','-'), replacement = '')),\r\n                logcatch  = log10(catch)) # create a  new variable of log catch\r\n\r\n#### Filter\r\n\r\nRemove unwanted columns and observations.\r\n\r\n    # remove the \"Totals\" values and any years with NA catch values\r\n    d<-filter(d,!(country %in% c('Totals - Quantity (number)','Totals - Quantity (tonnes)')) & !is.na(catch))\r\n\r\n    # print data\r\n    d\r\n\r\n### Piping and chaining code\r\n\r\nWhile the above workflow is perfectly acceptable, dplyr allows you to\r\nuse the *pipe* (`%>%`) operator to *chain* functions together. Chaining\r\ncode allows you to streamline your workflow and make it easier to read.\r\n\r\nWhen using the `%>%` operator, first specify the data frame that all\r\nfollowing functions will use. For the rest of the chain the data frame\r\nargument can be omitted from the remaining functions.\r\n\r\nNow consider the same process as before only using pipes and a single\r\ndplyr chain:\r\n\r\n    d <- fao %>%\r\n      gather(key='Year',value = 'Catch',num_range('X',1950:2012)) %>% # convert to long format\r\n      rename(\r\n        country     = Country..Country., # rename columns\r\n        #country     = `Country (Country)`, # backtick trick!\r\n        commname    = Species..ASFIS.species.,\r\n        spcode      = Species..ASFIS.species..1,\r\n        sciname     = Species..ASFIS.species..2,\r\n        spgroup     = Species..ISSCAAP.group.,\r\n        spgroupname = Species..ISSCAAP.group..1,\r\n        regionfao   = Fishing.area..FAO.major.fishing.area.,\r\n        unit        = Measure..Measure.,\r\n        year        = Year,\r\n        catch       = Catch) %>%\r\n      select(-spgroup,-unit) %>% # drop spgroup, regionfaoname, and unit variables\r\n      arrange(country,sciname,regionfao,year) %>% # order by country, sciname, regionfao, and year\r\n      mutate(\r\n        year        = as.numeric(str_replace(year, 'X', '')), # strip X off all year values and convert to numeric\r\n        catch       = as.numeric(gsub(catch, pattern=c(' F'), replacement = '', fixed = T)),\r\n        logcatch    = log10(catch)) %>% # create a  new variable of log catch \r\n      filter(!country %in% c('Totals - Quantity (number)','Totals - Quantity (tonnes)') & !is.na(catch)) # remove 'Totals' rows - rows: 1,114,596 -> 310,619\r\n\r\n    ## Warning in eval(substitute(expr), envir, enclos): NAs introduced by\r\n    ## coercion\r\n\r\n    # print data frame\r\n    d\r\n\r\nBy chaining our code we were able to reproduce the same data frame\r\nwithout the need to continually overwrite it, and we can easily read\r\neach step in the process by observing the different verbs. We also only\r\nneeded to reference the original data frame (fao) at the beginning of\r\nthe chain rather than in each function call.\r\n\r\n#### Complete\r\n\r\nNow our data is nice and tidy, but we realize that we actually want to\r\nretain NA values for years with missing catch data. We could just go\r\nback and remove the second argument from our `filter()` function. Or we\r\ncould use the nifty `complete()` function to add in the missing\r\ncombinations.\r\n\r\n    d %>%\r\n      complete(year = 1950:2012)\r\n\r\n    d %>%\r\n      group_by(country,sciname,commname,regionfao,spgroupname,spcode) %>%\r\n      complete(year = 1950:2012) %>%\r\n      ungroup()\r\n\r\n#### Separate and Unite\r\n\r\nThe `df$spcode` variable actually consists of 5 individual parts.\r\n\r\n![](spcodes.png)\r\n\r\nWe decide we want to create a new column for each taxonomic division of\r\nthe spcode. We can accomplish this with `separate()` and undue it with\r\n`unite()`\r\n\r\n    # create new variables for each taxonomic component \r\n    d<-separate(d,spcode, into = c('maintaxa','order','family','genus','species'), sep = c(2,4,6,9))\r\n\r\n    # recombine the columns with unite \r\n    d<-unite(d, col = spcode, maintaxa:species, sep = '') # Note - we can use helper functions here if needed\r\n\r\n### Joins\r\n\r\nSo far we've been working with a single data frame, but dplyr provides a\r\nhandful of really useful **join** functions that allow you to combine\r\ndatasets in a variety of ways. To demonstrate the different methods of\r\njoining, we will combine our FAO dataset with a dataset of life history\r\ninformation from FishBase.\r\n\r\nDplyr allows for *mutating* joins and *filtering* joins. Mutating joins\r\nwill combine information from both data frames in different ways, while\r\nfiltering joins will filter a single dataset based on matches in another\r\ndata set.\r\n\r\nFor joins to work, variable names must be the same in both datasets.\r\nThis often requires using `rename()` prior to your join functions if you\r\ndo not want to permanently alter the variable names in each dataset.\r\n\r\n-   Mutating joins\r\n-   `left_join(a, b, by = c('...'))` join matching rows from b to a by\r\n    matching variables in vector  \r\n-   `right_join(a, b, by = c('...'))` join matching rows from a to b by\r\n    matching variables in vector  \r\n-   `inner_join(a, b, by = c('...'))` join data, retaining only rows in\r\n    both a and b  \r\n-   `full_join(a, b, by = c('...'))` join data, retaining all values,\r\n    all rows\r\n\r\nLets use join functions to explore adding life history parameters to our\r\nFAO data\r\n\r\n    # read in life history data\r\n    load(file = 'data/mpack.Rdata')\r\n    lh<-mpack$lh\r\n    rm(mpack)\r\n\r\n    lh<-lh %>%\r\n      tbl_df() %>%\r\n      dplyr::rename(sciname=sname) %>% # rename to sciname for joining\r\n      select(sciname,vbk,temp,maxl,agem) %>% # select variables we wish to add\r\n      slice(match(unique(lh$sname),lh$sname))\r\n\r\n    # first let's pull out all species US fisheries\r\n    us<- d %>%\r\n      ungroup() %>%\r\n      filter(country=='United States of America' & year==2012) %>%\r\n      select(country, sciname, commname, spgroupname) %>%\r\n      distinct()\r\n      \r\n    # left join to retain all data in our d data frame. \r\n    us %>% \r\n      left_join(lh, by = 'sciname') # we only need to specify the right hand data set to join lh with since we've piped\r\n\r\n    # right join to keep all lh data.  \r\n    us %>%\r\n      right_join(lh, by = 'sciname')\r\n\r\n    # inner join to only keep data for which we have matches in both data sets\r\n    us %>%\r\n      inner_join(lh, by = 'sciname')\r\n\r\n    # full join to keep all data for both data sets\r\n    us %>%\r\n      full_join(lh, by = 'sciname')\r\n\r\nAnalyzing and Manipulating Data\r\n-------------------------------\r\n\r\nNow that we have our cleaned data in a tidy format let's do some\r\nanalyses. First, here are a few more simple examples of chaining code to\r\nselect, filter, and arrange our data to obtain different subsets.\r\n\r\n    # Canada's fisheries from largest to smallest in 2012\r\n    d %>%\r\n      filter(country=='Canada' & year==2012) %>%\r\n      select(year,country,commname,catch) %>%\r\n      arrange(desc(catch))\r\n\r\n    # All fisheries in the Northwest Atlantic with a catch over 1000 MT\r\n    d %>%\r\n      filter(regionfao==21 & year==2012 & catch>=1000) %>%\r\n      select(country,commname,regionfao,catch) %>%\r\n      arrange(desc(catch))\r\n\r\n    # Which countries have the 10 largest shark fisheries?\r\n    d %>%\r\n      filter(spgroupname=='Sharks, rays, chimaeras' & year==2012) %>%\r\n      select(country,commname,catch) %>%\r\n      arrange(desc(catch)) %>%\r\n      slice(1:10)\r\n\r\n### Grouping, Summarizing, and Mutating Data\r\n\r\nDplyr uses two main verbs to analyze data, `summarize()` and `mutate()`.\r\nSummary functions will summarize data two produce a single row of output\r\nwhile mutate functions create a new variable the same length as the\r\ninput data. For both functions, you first indicate the name of the\r\nvariable that will be created and then specify the calculation to be\r\nperformed.\r\n\r\n-   Example: `totalcatch=sum(catch,na.rm=T)`\r\n\r\n![](sum_mutate.png)\r\n\r\nThe `group_by()` function lets you specify the level across which to\r\napply your calculations.\r\n\r\n-   A key thing to remember is to always `ungroup()` your data if you\r\n    intend to perform additional calculations, as grouped data frames\r\n    can result in incorrect results downstream if performed at\r\n    different levels.\r\n\r\n![](group_by.png)\r\n\r\nUsing `group_by()` and `summarize()` let's calculate total global\r\nharvest from 1950 to 2012 for several groups of data\r\n\r\n    # Total global harvest\r\n    global <- d %>%\r\n      ungroup() %>%\r\n      group_by(year) %>%\r\n      dplyr::summarize(totalcatch=sum(catch,na.rm=T)) %>%\r\n      ggplot(aes(x=year,y=totalcatch)) +\r\n      geom_line()\r\n\r\n    # Global harvest by country\r\n    cntry<-d %>%\r\n      group_by(year,country) %>%\r\n      dplyr::summarize(totalcatch=sum(catch, na.rm=T)) %>%\r\n      ungroup() %>% # -- Here's an example of why you need to ungroup! --\r\n      dplyr::arrange(country)\r\n\r\n    # Global harvest by species category\r\n    spcatch <- d %>%\r\n      group_by(year,spgroupname) %>%\r\n      dplyr::summarize(totalcatch=sum(catch, na.rm=T)) %>%\r\n      ungroup() %>% \r\n      arrange(spgroupname)\r\n\r\n    # USA harvest by species category over time\r\n    usa<- d %>%\r\n      filter(country=='United States of America') %>%\r\n      group_by(year,country,spgroupname) %>%\r\n      dplyr::summarize(totalcatch=sum(catch,na.rm=T)) %>%\r\n      ungroup() %>%\r\n      arrange(spgroupname)\r\n\r\nNow let's use mutate to calculate some additional information for our\r\ndatasets\r\n\r\n    # Calculate what % of global catch each country contributes in each year and for rank each year by that %\r\n    cntry %>%\r\n      group_by(year) %>%\r\n      mutate(\r\n        globalcatch = sum(totalcatch,na.rm=T),\r\n        globalrank  = dense_rank(totalcatch)) %>% # global catch and cntry rank\r\n      group_by(year,country) %>% # now we group by a different level before our next calculation\r\n      mutate(\r\n        percglobal = 100*(totalcatch/globalcatch)) %>%\r\n      group_by(country) %>%\r\n      mutate(\r\n        ingrouprank = dense_rank(totalcatch))\r\n\r\n### Using Dplyr with `broom` and `ggplot2`\r\n\r\nOne of the best aspects of working with tidy data and `dplyr` is how\r\neasy it makes it to quickly manipulate and plot your data. Property\r\norganized, it's a piece of cake to quickly make summaries and plots of\r\nyour data without making all kinds of \"temporary\" files or lines of\r\nspaghetti code for plotting. You can also basically eliminate loops from\r\nyour coding for all situations except that those that require dynamic\r\nupdating (e.g. population models).\r\n\r\nFor this next exercise, we're going to use `tidyr`, `dplyr`, `broom`,\r\nand `ggplot2` to fit a model, run diagnostics, and plot results.\r\n\r\nIt's 3am. You've been chasing the same cryptic error message for two\r\ndays (<font color = 'red'>\r\n`Error: towel not found, don't panic!`</font>). You decide enough is\r\nenough: you're going to pack it in, buy a boat and become a fisherman.\r\nThe only problem is, years of coding have left you with no knowledge of\r\nthe outside world besides what R and data can tell you. How are you\r\nsupposed to know what to fish for, or where to fish? Luckily, you have\r\nsome data, so you turn to your laptop one last time before hurling it\r\noff of a cliff in a ritualistic sacrifice to the sea gods.\r\n\r\nYou want to find a fishery to join based on two criteria: high average\r\ncatch, and low average variability. You might now know these data\r\nthough, so you want to be able to predict what fishery to join based on\r\ngeographic and life history traits.\r\n\r\nOur first goals:\r\n\r\n1.  Generate a unique ID for each fishery\r\n\r\n2.  Calculate the mean log lifetime catch of each fishery\r\n\r\n3.  Calculate the coefficient of variation of each fishery\r\n\r\n4.  Filter out fisheries with short time series\r\n\r\n<!-- -->\r\n\r\n    # Prep our data\r\n    dat <- d %>%\r\n      ungroup() %>% #Often a good idea to ungroup before starting something new\r\n      mutate(id = paste(country,spcode,regionfao, sep = '_')) %>% #Generate a unique ID for each fishery\r\n      group_by(id) %>%\r\n      mutate(mean_log_catch = mean(logcatch, na.rm = T), cv_log_catch = sd(logcatch, na.rm = T)/mean(logcatch, na.rm = T), length_catch = sum(is.na(logcatch) == F & logcatch >0)) %>% # we want to keep some of the other data as well\r\n      filter(year == max(year) & length_catch > 10 & is.finite(mean_log_catch) == T & cv_log_catch >0) %>% # We don't want repeated entries, so let's just grab one random year\r\n      dplyr::select(-year, -catch, -logcatch)\r\n\r\n    # Always plot!\r\n    ggplot(dat, aes(mean_log_catch,cv_log_catch)) + \r\n      geom_point()\r\n\r\n![](README_files/figure-markdown_strict/unnamed-chunk-17-1.png)<!-- -->\r\n\r\nOK, we see we're onto something here: there's clearly a relationship\r\nbetween average catch and the CV of the catch. We want to build a model\r\nthat predicts that. Let's create a composite score of the mean log catch\r\nand the inverse of the CV. We're going to scale the log catches by the\r\nmaximum log catch, and the CV by the the maximum of 1/CV. We also want\r\nto add in our nice life history data\r\n\r\n    regdat <-  dat %>%\r\n      ungroup() %>% #we want global statistics now\r\n      mutate(scaled_ml_catch = mean_log_catch/max(mean_log_catch), scaled_cv_catch =  (cv_log_catch/min(cv_log_catch))^-1, fishiness = scaled_ml_catch + scaled_cv_catch) %>%\r\n      left_join(lh, by = 'sciname')\r\n\r\n    regplot <- regdat %>% #great thing about ggplot is the ability to save as an object and use and modify later\r\n      ggplot(aes(mean_log_catch,cv_log_catch, fill = fishiness)) + \r\n      geom_point(shape = 21) + \r\n      scale_fill_gradient(low = 'red',high = 'green')\r\n\r\n    regplot # grea\r\n\r\n![](README_files/figure-markdown_strict/unnamed-chunk-18-1.png)<!-- -->\r\n\r\nNow we're getting somewhere! Now, lets run a regression using life\r\nhistory and geographic variables to try and predict the quality of\r\nfishing.\r\n\r\n    reg_vars <- c('regionfao', 'spgroupname', 'vbk','maxl','temp') #specify variables you want\r\n\r\n    class(regdat$regionfao) #whoops, it things FAO region is an integer, we want a factor\r\n\r\n    filtered_dat <- regdat %>%\r\n      ungroup() %>%\r\n      mutate(has_all = apply(is.na(regdat[,reg_vars]) == F, 1,all)) %>%\r\n      filter(has_all == T) %>%\r\n      mutate(regionfao = as.factor(regionfao),spgroupname = as.factor(spgroupname))\r\n\r\n    reg_fmla <- as.formula(paste('fishiness ~',paste(reg_vars, collapse = '+'), sep = '')) #create regression formula\r\n\r\n    fish_model <- lm(reg_fmla, data = sample_n(filtered_dat, 10000, replace = T)) #run a linear regression\r\n    summary(fish_model)\r\n\r\nNow we've got a model! we're close to being able to use data to predict\r\nwhere we'll start our fishing operation. But, while we know nothing\r\nabout fishing, we are good statisticians, and we know we should look at\r\nour regression before using it to make a big life decision. This is\r\nwhere `broom` comes in. R has all kinds of great functions, like\r\n`summary()` to look at regressions. But, they can be a little ad hoc,\r\nand difficult to manipulate. `broom` helps us tidy up our regression\r\ndata. First, suppose that we want a better way to look at summary\r\nstatistics from the regression. The `glance()` function from the `broom`\r\npackage extracts important summary statistics from the model, like the\r\n*R<sup>2</sup>*, the AIC, and the BIC.\r\n\r\n    library(broom)\r\n    reg_summary <- glance(fish_model)\r\n\r\n    reg_summary\r\n\r\nUnfortunately, our model is pretty poor; it only explains ~20% of the\r\nvariation in the `fishiness` variable, but hopefully it's better than\r\nguessing. Let's dig into this model a bit more. We're going to use the\r\n`tidy()` function from the `broom` package to provide neat summaries of\r\nthe model coefficients.\r\n\r\n    tidy_model <- tidy(fish_model)\r\n\r\n    tidy_model$variable<- as.factor(tidy_model$term) #convert terms to factors\r\n\r\n    tidy_model$variable <- reorder(tidy_model$variable, tidy_model$p.value) #sort variables by pvalue\r\n\r\n    tidy_model$short_pval<- pmin(tidy_model$p.value,0.2) #create abbreviated version\r\n\r\n    regression_plot <- (ggplot(data=tidy_model,aes(x=variable,y=estimate,fill=short_pval))+\r\n                          geom_bar(position='dodge',stat='identity',color='black')+\r\n                          scale_fill_gradient2(high='black',mid='gray99',low='red',midpoint=0.1,\r\n                                               breaks=c(0.05,0.1,0.15,0.2),labels=c('0.05','0.10','0.15','>0.20')\r\n                                               ,name='P-Value',guide=guide_colorbar(reverse=T))\r\n                        +theme(axis.text.x=element_text(angle=45,hjust=0.9,vjust=0.9))+\r\n                          geom_errorbar(mapping=aes(ymin=estimate-1.96*std.error,ymax=estimate+1.96*std.error))+\r\n                          xlab('Variable')+\r\n                          ylab(paste('Marginal Effect on ',names(fish_model$model)[1],sep='')) + \r\n                          coord_flip())\r\n\r\n    regression_plot\r\n\r\n![](README_files/figure-markdown_strict/unnamed-chunk-21-1.png)<!-- -->\r\n\r\nSo, we can now see that most of the significant terms are region\r\nspecific, and the life history data doesn't give us a whole lot of\r\ninformation on where we should start fishing. So far, the model is\r\nsaying go fish in China, and maybe avoid salmons, halibuts, and tunas.\r\n\r\nBefore we charge off and use these results though to decide where we're\r\nstarting our new life, we're now going to use the `augment()` function\r\nin the `broom` package to help us run some diagnostics on the\r\nregression. The `augment` function takes our original data passed to the\r\nregression, and adds all kinds of things, like the values predicted by\r\nthe model and the residuals. This makes it very useful for regression\r\ndiagnostics. First off, we might want to check whether our errors are\r\nactually normally distributed\r\n\r\n    auged_reg <- augment(fish_model)\r\n\r\n\r\n    obs_v_pred <- auged_reg %>%\r\n      ggplot(aes(fishiness, .fitted)) + \r\n      geom_point(shape = 21, size = 4, alpha = 0.6, fill = 'steelblue4') + \r\n      geom_abline(aes(slope=1, intercept = 0)) + \r\n      xlab('ovbserved') + \r\n      ylab('predicted') + \r\n      geom_label(aes(0.25,0.7), label = paste('R2 = ', round(reg_summary$r.squared,2), sep = ''))\r\n\r\n    obs_v_pred\r\n\r\n![](README_files/figure-markdown_strict/unnamed-chunk-22-1.png)<!-- -->\r\n\r\n      qq_plot <- auged_reg %>% #create quantile-quantile plot\r\n        ggplot(aes(sample = .resid)) +\r\n        stat_qq(shape = 21, size = 4, alpha = 0.6, fill = 'steelblue4') +\r\n        xlab('Theoretical') +\r\n        ylab('Sample')\r\n\r\n      qq_plot\r\n\r\n![](README_files/figure-markdown_strict/unnamed-chunk-22-2.png)<!-- -->\r\n\r\nWe see that our data are in fact normally distributed, that's good!\r\nLet's check for heteroskedasticity and model misspecification.\r\n\r\n      hetsk_plot <- auged_reg %>% #plot fitted vs residuals\r\n        ggplot(aes(.fitted, .resid)) +\r\n        geom_point(shape = 21, size = 4, alpha = 0.6, fill = 'steelblue4') +\r\n      geom_hline(aes(yintercept = 0)) + \r\n        xlab('Predicted') +\r\n        ylab('Residuals')\r\n\r\n    hetsk_plot\r\n\r\nLooks a little iffy, we've got some heteroskedasticity going on. Let's\r\ntry and see where it is. The great thing about `broom` is that it makes\r\nit really easy to manipulate data and plot diagnostics based on the\r\noriginal data.\r\n\r\n      hetsk_plot2 <- auged_reg %>% \r\n        ggplot(aes(.fitted, .resid, fill = spgroupname)) +\r\n        geom_point(shape = 21, size = 4, alpha = 0.6) +\r\n      geom_hline(aes(yintercept = 0)) + \r\n        xlab('Predicted') +\r\n        ylab('Residuals')\r\n\r\n    hetsk_plot2\r\n\r\n![](README_files/figure-markdown_strict/unnamed-chunk-24-1.png)<!-- -->\r\n\r\nSo, we see here that the culprit are the herrings and salmons. That\r\ntells us to be a little cautious in our predictive ability and estimated\r\nerrors based on this model, and maybe we need to do a better job of\r\nclustering our errors. Let's look at things another way. We saw from the\r\ncoefficient plot that the region effects are the most significant in the\r\nmodel. How confident are we in those?\r\n\r\n      regional_bias <- auged_reg %>% #Check residuals by group\r\n      ggplot(aes(regionfao,.resid)) + \r\n      geom_boxplot(fill = 'steelblue4') + \r\n      geom_hline(aes(yintercept = 0)) + \r\n      xlab('FAO Region') + \r\n      ylab('Residuals')\r\n\r\n    regional_bias\r\n\r\n![](README_files/figure-markdown_strict/unnamed-chunk-25-1.png)<!-- -->\r\n\r\n      species_bias <- auged_reg %>%\r\n      ggplot(aes(spgroupname,.resid)) + \r\n      geom_boxplot(fill = 'steelblue4') + \r\n      geom_hline(aes(yintercept = 0)) + \r\n      xlab('Species Category') + \r\n      ylab('Residuals') + \r\n        coord_flip()\r\n      \r\n      species_bias\r\n\r\n![](README_files/figure-markdown_strict/unnamed-chunk-25-2.png)<!-- -->\r\n\r\nAll in all then, we've got some heteroskedasticity that makes us a\r\nlittle suspicious of our standard errors, but no major biases in our\r\nestimation. Our life choice model works! Let's move to China and fish\r\nwhatever, the model says it doesn't matter.\r\n\r\n### In defense of `plyr`\r\n\r\nOne quick note. `dplyr` has taken over for a lot of the things we used\r\nto use `plyr` for. But, `plyr` is still useful for manipulating other\r\ntypes of objects instead of data frames. Specifically, I use `plyr` to\r\nconvert lists and arrays to data frames.\r\n\r\nSometimes its useful to use lists. Suppose that I have a function that I\r\nwant to evaluate a bunch of times. Loops can be cumbersome for a variety\r\nof reasons. Let's write a function and apply it over a vector instead.\r\n\r\n    foo <- function(x){ #random function\r\n      \r\n      y <- x^2\r\n      \r\n      return(y)\r\n    }\r\n\r\n    food <- lapply(1:100,foo) #this can be more efficient and simpler than loops\r\n\r\nNow, we've applied our function over 100 values. But, they're stuck in\r\nlist form. `plyr` to the rescue! So long as each element in every list\r\nhas the same dimensions, `ldply` will \"smash\" the list into a data frame\r\n\r\n    foody <- plyr::ldply(food)\r\n\r\nThe syntax is simple. `ldply` converts lists to data frames. `adply`\r\nconverts arrays to data frames. You get the idea. Huge warning here.\r\n**Make sure** you load the `plyr` library **before** `dplyr`. Otherwise,\r\nbad bad things can happen. R will even throw a warning if you do it the\r\nother way around. Or, more simply, instead of loading the library, just\r\nuse `plyr::ldply`. This loads that function for that instance, without\r\nactually loading into the environment and masking other things.\r\n\r\n### A quick warning on speed\r\n\r\nSo far, we've been preaching the `dplyr` gospel pretty hard. All in all,\r\nit makes code faster, more efficient, and much easier to read. But,\r\nthere are times when its best to keep it simple, especially where speed\r\nis critical. This is less `dplyr`'s fault, than some issues with data\r\nframes themselves.\r\n\r\nWe are going to compare two functions that do the same thing, one using\r\ndplyr and data frames and one that uses more basic R functions. The goal\r\nis a function that calculates the mean length of catch history in an fao\r\nregion\r\n\r\n    dplyr_fun <- function(region,dat)\r\n    {\r\n      out <- dat %>%\r\n        filter(regionfao == region) %>%\r\n        summarise(mean_length = mean(length_catch))\r\n      \r\n      return(out)\r\n    }\r\n\r\n    basic_fun <- function(region,dat)\r\n    {\r\n      out <- mean(as.numeric(dat[dat[,'regionfao'] == region,'length_catch']))\r\n      return(out)\r\n    }\r\n\r\n    regions <- rep(unique(as.character(regdat$regionfao)), 100) #thing to test\r\n\r\n    startime <-  proc.time() #time the dplyr version\r\n    a <- lapply(regions, dplyr_fun, dat = regdat)\r\n    t1 <- proc.time() - startime\r\n\r\n    startime <-  proc.time() #time the basic version\r\n    b <- lapply(regions, basic_fun, dat = as.matrix(regdat))\r\n    t2 <- proc.time() - startime\r\n\r\n    t1[1]/t2[1]\r\n\r\n    ## user.self \r\n    ##   6.82449\r\n\r\n    all(plyr::ldply(a)$V1 == plyr::ldply(b)$V1) #check and make sure they do the same thing\r\n\r\n    ## [1] TRUE\r\n\r\nThe `dplyr` version of the function takes nearly 7 times as long as the\r\nsame function in basic notation! The difference between .45 and 3.1\r\nseconds doesn't matter much in most cases, but if you're doing huge\r\nnumbers of simulations, say in an MCMC, this starts to add up. This can\r\nbe the difference between a model running a day and a few hours.\r\n\r\nThis time sink doesn't always hold true, `dplyr` will often be faster\r\nthan bunches of nested loops, but when speed is a priority, it's worth\r\nchecking to see using matrices instead of data frames and `dplyr` will\r\nsave you some serious time.\r\n\r\nAdvanced Dplyr Applications\r\n---------------------------\r\n\r\n### Underscore Functions\r\n\r\nOften, when writing functions with dplyr we may want to be able to\r\nspecify different grouping variables. But wait, dplyr arguments use\r\nunquoted variable names! Have no fear, underscore is here!\r\n\r\nCheck out the following two functions:\r\n\r\n    # function using standard dplyr functions\r\n    fun1<-function(x,gpvar1,gpvar2,gpvar3){\r\n      y<-x %>%\r\n        group_by(gpvar1) %>%\r\n        mutate(globalcatch=sum(totalcatch,na.rm=T),globalrank=dense_rank(totalcatch)) %>% # global catch and cntry rank\r\n        group_by(gpvar2) %>% # now we group by a different level before our next calculation\r\n        mutate(percglobal=100*(totalcatch/globalcatch)) %>%\r\n        group_by(gpvar3) %>%\r\n        mutate(ingrouprank=dense_rank(totalcatch))\r\n      return(y)\r\n    }\r\n\r\n    fun1(spcatch, gpvar1 = year, gpvar2 = c(year,country), gpvar3 = country) # !!!!! THIS WILL NOT WORK !!!!!\r\n\r\n    # function using underscores\r\n    fun1<-function(x,gpvar1,gpvar2,gpvar3){\r\n      y<-x %>%\r\n        group_by_(gpvar1) %>%\r\n        mutate(globalcatch=sum(totalcatch,na.rm=T),globalrank=dense_rank(totalcatch)) %>% \r\n        group_by_(gpvar2) %>% \r\n        mutate(percglobal=100*(totalcatch/globalcatch)) %>%\r\n        group_by_(gpvar3) %>%\r\n        mutate(ingrouprank=dense_rank(desc(totalcatch)))\r\n      return(y)\r\n    }  \r\n\r\n    # apply function to species category and country datasets\r\n    spcatch<-fun1(spcatch,gpvar1 = c('year'), gpvar2 = c('year','spgroupname'), gpvar3 = c('spgroupname')) \r\n    cntry<-fun1(cntry,gpvar1 = c('year'), gpvar2 = c('year','country'), gpvar3 = c('country'))   \r\n\r\n### Using Dplyr to Query External Databases\r\n\r\nNeed to setup Google account first, per [A new data processing workflow\r\nfor R: dplyr, magrittr, tidyr, ggplot2 | Technical Tidbits From Spatial\r\nAnalysis & Data\r\nScience](http://zevross.com/blog/2015/01/13/a-new-data-processing-workflow-for-r-dplyr-magrittr-tidyr-ggplot2/).\r\n\r\n    # library(dplyr)\r\n    library(bigrquery) # install.packages('bigrquery')\r\n    sql<-\"select * from [publicdata:samples.shakespeare]\"\r\n    shakespeare <-query_exec(sql, project =\"test-bigquery-1243\",max_pages=Inf)\r\n\r\n#### Example Analyses\r\n\r\n-   [NOAA Storm Data - a Brief Analysis of Impact on Health and\r\n    Economy](http://rstudio-pubs-static.s3.amazonaws.com/25871_1ffdd88781bd4e6194d93fd327a25659.html)\r\n-   [dplyr example with fish data](http://rpubs.com/dogle/31773) (more\r\n    on [Introductory Fisheries Analysis with R |\r\n    fishR](https://fishr.wordpress.com/ifar/))\r\n-   [rnoaa - Access to NOAA National Climatic Data Center\r\n    data](https://ropensci.org/blog/2014/03/13/rnoaa/)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}